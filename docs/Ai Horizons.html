<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Horizon: Open vs. Closed LLMs - 70s Autumn Retro Edition (Fixed)</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=VT323&family=Roboto+Mono:wght@400;700&display=swap" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/3.9.1/chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns/dist/chartjs-adapter-date-fns.bundle.min.js"></script>

    <style>
        :root {
            /* 70s Autumn Theme Palette */
            --bg-color-page: #E6D8C8; /* Very light warm beige */
            --bg-color-window: #FAF0E6; /* Linen - warmer white for windows */
            --text-primary: #2F1B0C; /* Dark Brown for text */
            --text-secondary: #593B1F; /* Medium Brown for secondary text */
            --accent-highlight: #2F1B0C; /* Dark brown for active tab text/borders */
            --border-color: #4A2F1A; /* Darker brown for borders */
            --button-bg: #E0D6C8; /* Slightly darker beige for buttons */
            --button-active-bg: #D0C0B0; /* Even darker for active buttons */
            --title-bar-stripe-color: #C0B0A0; /* Muted brown for window title bar stripes */
            --modal-bg: rgba(47, 27, 12, 0.6); /* Semi-transparent dark brown for modal background */
            --modal-content-bg: #FAF0E6; /* Linen for modal content */

            --font-chart-label: 'Arial', sans-serif; 
            --chart-grid-color-theme: #D1C7BC; /* A slightly darker, muted beige for grid lines */
            
            /* Chart Colors - 70s Autumn Mix */
            --chart-color-open-pri: #FF8C00;   /* DarkOrange */
            --chart-color-open-sec: #FFA07A;   /* LightSalmon (Lighter Orange/Peach) */
            --chart-color-closed-pri: #8B4513; /* SaddleBrown */
            --chart-color-closed-sec: #A0522D; /* Sienna (Medium Brown) */
            --chart-color-projected: #FFD700;  /* Gold (Golden Yellow) */
            --chart-color-misc-1: #D2691E;    /* Chocolate (Darker Orange/Brown) */
            --chart-color-misc-2: #D2B48C;    /* Tan (Lighter Brown/Beige) */
            
            --font-heading: 'VT323', monospace;
            --font-subheading: Arial, Helvetica, sans-serif;
            --font-body: Arial, Helvetica, sans-serif;
            --font-data: 'Roboto Mono', monospace;
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body { font-family: var(--font-body); background-color: var(--bg-color-page); color: var(--text-primary); line-height: 1.5; padding-top: 45px; }
        nav { background-color: var(--bg-color-page); padding: 5px 0; text-align: center; position: fixed; top: 0; width: 100%; z-index: 1000; border-bottom: 1px solid var(--border-color); display: flex; flex-wrap: wrap; justify-content: center; }
        nav button { background-color: var(--button-bg); border: 1px solid var(--border-color); color: var(--text-primary); padding: 5px 9px; margin: 2px; font-family: var(--font-subheading); font-size: 0.72em; cursor: pointer; box-shadow: 1px 1px 0px 0px rgba(47,27,12,0.4); }
        nav button.active, nav button:hover { background-color: var(--button-active-bg); color: var(--accent-highlight); box-shadow: inset 1px 1px 0px 0px rgba(47,27,12,0.2); }
        nav button.active { font-weight: bold; }
        .tab-content { display: none; background-color: var(--bg-color-window); border: 1px solid var(--border-color); box-shadow: 2px 2px 0px 0px #888888, 4px 4px 0px 0px #555555; margin: 20px auto; width: 95%; max-width: 1320px; animation: fadeIn 0.3s; padding-bottom: 20px; }
        .tab-content.active { display: block; }
        .mac-window-title-bar { background-color: var(--bg-color-window); border-bottom: 1px solid var(--border-color); padding: 2px 8px; height: 24px; display: flex; align-items: center; position: relative; }
        .mac-window-title-bar::before { content: ""; position: absolute; top: 4px; left: 5px; right: 5px; bottom: 4px; background-image: repeating-linear-gradient(to bottom, transparent, transparent 2px, var(--title-bar-stripe-color) 2px, var(--title-bar-stripe-color) 3px); z-index: 0; opacity: 0.6; }
        .mac-window-title { font-family: var(--font-heading); font-size: 1.35em; color: var(--text-primary); text-align: center; flex-grow: 1; position: relative; z-index: 1; letter-spacing: 1px; line-height: 1; margin-top: 2px; }
        @keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } }
        .content-wrapper { padding: 20px 3%; }
        .container { display: flex; gap: 25px; margin-top: 20px; }
        .main-content { flex: 2.7; } 
        .sidebar { flex: 1.3; padding: 15px; background-color: #EFEBE4; border: 1px solid var(--border-color); } /* Slightly different sidebar bg */
        .sidebar-item { margin-bottom: 18px; padding: 10px; background-color: #FAF0E6; border: 1px solid #C1B8AD; } /* Warmer border for sidebar items */
        .sidebar-item h4 { font-family: var(--font-subheading); font-weight: bold; color: var(--text-primary); margin-bottom: 8px; font-size: 0.78em; text-transform: uppercase; letter-spacing: 0.5px; }
        .sidebar-item p, .sidebar-item ul { font-size: 0.73em; color: var(--text-secondary); }
        .sidebar-item ul { list-style-position: inside; padding-left: 5px; }
        .sidebar-item li { margin-bottom: 4px; }
        .graph-container { width: 100%; height: 230px; background-color: #FDFBF7; border: 1px solid var(--border-color); margin-top: 8px; padding: 8px; } /* Very light graph bg */
        h1 { display: none; }
        h2 { font-family: var(--font-heading); font-size: 1.65em; color: var(--text-primary); margin-bottom: 10px; border-bottom: 1px solid var(--border-color); padding-bottom: 3px; letter-spacing: 0.5px;}
        h3 { font-family: var(--font-subheading); font-size: 1.05em; font-weight: bold; color: var(--text-secondary); margin-top: 15px; margin-bottom: 7px;}
        p { margin-bottom: 10px; color: var(--text-primary); font-size: 0.83em; }
        table { width: 100%; border-collapse: collapse; font-size: 0.73em; margin-bottom: 12px; }
        th, td { border: 1px solid var(--border-color); padding: 3px; text-align: left; }
        th { background-color: var(--button-bg); font-weight: bold; }
        .report-header { text-align: center; margin-bottom: 25px; padding: 18px; background-color: #EFEBE4; border-bottom: 1px solid var(--border-color); } /* Warmer report header */
        .report-header p { font-size: 0.88em; color: var(--text-secondary); max-width: 800px; margin: 8px auto; }
        .report-header .meta-buttons button { background-color: var(--button-bg); color: var(--text-primary); border: 1px solid var(--border-color); padding: 6px 12px; margin: 4px; font-family: var(--font-subheading); font-size: 0.78em; cursor: pointer; box-shadow: 1px 1px 0px 0px rgba(47,27,12,0.4); }
        .report-header .meta-buttons button:hover { background-color: var(--button-active-bg); box-shadow: inset 1px 1px 0px 0px rgba(47,27,12,0.2); }
        .report-date { font-size: 0.73em; color: var(--text-secondary); margin-top: 12px; font-family: var(--font-data); }
        .footer { text-align: center; padding: 20px; margin-top: 30px; border-top: 1px solid var(--border-color); font-size: 0.68em; color: var(--text-secondary); background-color: var(--bg-color-page); }
        .footer p { font-size: 0.78em; margin-bottom: 4px; }
        .modal { display: none; position: fixed; z-index: 2000; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; background-color: var(--modal-bg); padding-top: 50px; }
        .modal-content { background-color: var(--modal-content-bg); margin: 5% auto; padding: 22px; border: 2px solid var(--border-color); width: 65%; max-width: 750px; font-family: var(--font-body); font-size: 0.88em; box-shadow: 3px 3px 0px 0px #888888, 6px 6px 0px 0px #555555; }
        .modal-content h3 { font-family: var(--font-heading); font-size: 1.5em; margin-bottom: 12px; color: var(--text-primary); border-bottom: 1px solid var(--border-color); padding-bottom: 4px;}
        .modal-content p {font-size: 0.83em; margin-bottom:8px;}
        .modal-content ul { list-style-type: square; padding-left: 22px; font-size: 0.83em; }
        .modal-content li { margin-bottom: 6px; }
        .close-button { color: var(--text-secondary); float: right; font-size: 26px; font-weight: bold; cursor: pointer; line-height: 0.8; padding: 0 4px; border: 1px solid transparent; }
        .close-button:hover, .close-button:focus { color: var(--text-primary); background-color: var(--button-bg); border: 1px solid var(--border-color); text-decoration: none; }
        @media (max-width: 1024px) { .main-content { flex: 2.5; } .sidebar { flex: 1.5; } .graph-container { height: 200px; } }
        @media (max-width: 768px) { body { padding-top: 70px; } .container { flex-direction: column; } nav button { font-size: 0.68em; padding: 4px 8px;} .mac-window-title { font-size: 1.15em; } h2 { font-size: 1.35em; } .tab-content { width: 95%; max-width: 95%; } .graph-container { height: 230px; } .main-content, .sidebar { flex: none; } .modal-content { width: 90%; margin: 10% auto;} }
        @media (max-width: 480px) { body { padding-top: 85px; } nav button { padding: 4px 6px; font-size: 0.65em;} .graph-container { height: 250px; } }
    </style>
</head>
<body>
    <div id="threejs-bg"></div>

    <nav>
        <button class="tab-button active" onclick="openTab(event, 'foundations')">AI Foundations</button>
        <button class="tab-button" onclick="openTab(event, 'capabilities')">Capabilities</button>
        <button class="tab-button" onclick="openTab(event, 'training')">Training & Data</button>
        <button class="tab-button" onclick="openTab(event, 'ecosystem')">Ecosystem</button>
        <button class="tab-button" onclick="openTab(event, 'applications')">Applications</button>
        <button class="tab-button" onclick="openTab(event, 'outlook')">Current Outlook</button>
        <button class="tab-button" onclick="openTab(event, 'projections')">2027 Projections</button>
    </nav>

    <!-- Modal Dialogs HTML (Content remains the same) -->
    <div id="reportFocusModal" class="modal">
        <div class="modal-content">
            <span class="close-button" onclick="closeModal('reportFocusModal')">×</span>
            <h3>Report Focus</h3>
            <p>This report provides an in-depth comparative analysis of open-source versus closed-source Large Language Models (LLMs). It examines their core architectures, capabilities, training methodologies, ecosystem dynamics, real-world applications, current challenges, and future projections towards 2027.</p>
            <p>The aim is to offer a data-informed perspective on the evolving LLM landscape, highlighting key trends, trade-offs, and the ongoing interplay between these two development paradigms. We assess aspects such as performance, accessibility, customization, cost, transparency, and ethical considerations.</p>
        </div>
    </div>
    <div id="ourStanceModal" class="modal">
        <div class="modal-content">
            <span class="close-button" onclick="closeModal('ourStanceModal')">×</span>
            <h3>Our Stance</h3>
            <p>Our analytical stance is one of objective evaluation, based on publicly available data, research findings, and reputable industry reports. We strive to present a balanced view, acknowledging the strengths and weaknesses inherent in both open-source and closed-source LLM development and deployment models.</p>
            <p>This report does not advocate for one approach over the other but rather aims to illuminate the complex factors shaping the field. We believe that both open and closed innovation contribute significantly to the advancement of AI, each with distinct roles and impacts on research, industry, and society.</p>
        </div>
    </div>
    <div id="dataSourcesModal" class="modal">
        <div class="modal-content">
            <span class="close-button" onclick="closeModal('dataSourcesModal')">×</span>
            <h3>Data Sources & Methodology</h3>
            <p>The information and data presented in this report are synthesized from a variety of reputable sources, including but not limited to:</p>
            <ul>
                <li>Peer-reviewed academic papers and pre-prints (e.g., from arXiv).</li>
                <li>Official model cards, technical reports, and blog posts from AI developers (e.g., OpenAI, Meta AI, Google AI, Anthropic, Mistral AI).</li>
                <li>Benchmark results from established leaderboards (e.g., MMLU, HumanEval, GSM8K).</li>
                <li>Industry analysis reports from organizations like Artificial Analysis, Stanford HAI (AI Index), Epoch AI.</li>
                <li>Technical articles and insights from reputable technology news outlets and specialized AI-focused publications.</li>
                <li>Publicly available datasets and code repositories (e.g., Hugging Face).</li>
            </ul>
            <p>Quantitative data (e.g., benchmark scores, parameter counts, training data sizes) are cited from these sources where available, with dates reflecting the latest known information (primarily mid-2024 to early 2025, with projections to 2027). Qualitative assessments and illustrative charts are based on synthesizing trends observed across multiple sources. Projections are extrapolations based on current trajectories and expert opinions within the field.</p>
            <p>This report uses a "data-informed, conceptual" approach; while specific numbers are drawn from real sources, some charts are "illustrative" to show trends rather than exact market shares or precise future values not yet known.</p>
        </div>
    </div>

    <!-- TAB 1: AI Foundations -->
    <div id="foundations" class="tab-content active">
        <div class="mac-window-title-bar"><span class="mac-window-title">AI Foundations: Open vs. Closed</span></div>
        <div class="content-wrapper">
            <h1>AI Foundations: Open vs. Closed Models - A Deep Dive</h1>
            <div class="report-header">
                <p><strong>The Artificial Intelligence landscape is characterized by a dynamic interplay between proprietary, closed-source Large Language Models (LLMs) from major corporations and rapidly advancing open-source alternatives. This report dissects their foundational aspects.</strong></p>
                <p>"We compare leading open-source LLMs like Meta's Llama 3 series, Mistral AI's models, Alibaba's Qwen2, Google's Gemma, and DeepSeek AI's models, against closed-source giants such as OpenAI's GPT series, Anthropic's Claude, and Google's Gemini. Analysis draws from public model cards, research papers, and industry reports."</p>
                <div class="meta-buttons">
                    <button onclick="openModal('reportFocusModal')">Report Focus</button>
                    <button onclick="openModal('ourStanceModal')">Our Stance</button>
                    <button onclick="openModal('dataSourcesModal')">Data Sources</button>
                </div>
                <p class="report-date">Data Updated: June 2025</p>
            </div>
            <div class="container">
                <article class="main-content">
                    <h2>Core Architectures & Access Models: The Transformer's Reign and the Openness Divide</h2>
                    <p>The Transformer architecture, particularly its decoder-only variants, continues to be the dominant backbone for both open-source (e.g., Llama 3, Qwen2, Mistral) and closed-source (e.g., GPT-4o, Claude 3 Opus, Gemini 1.5 Pro) LLMs. The primary divergence lies in access and transparency. Open models, like Llama 3 (Llama Community License), Mistral 7B (Apache 2.0), or Qwen2 (Tongyi Qianwen LICENSE), generally provide access to model weights, and often inference/fine-tuning code, fostering community-driven innovation and scrutiny. Closed models maintain proprietary control over weights and detailed architectures, offering access primarily through APIs. This distinction profoundly shapes their roles as "Foundation Models," with open versions providing an inspectable and adaptable base, while closed ones offer powerful but more opaque platforms.</p>
                    
                    <h2>Taxonomy: Incumbents, Challengers, and the Spectrum of Openness</h2>
                    <p>The LLM landscape is diverse, with models categorized by their development approach and access policies:</p>
                    <ul>
                        <li><strong>Closed-Source Frontier Models:</strong> Examples: OpenAI GPT-4o (~1.8T est. params, MMLU ~88.7%), Anthropic Claude 3 Opus (MMLU ~86.8%), Google Gemini 1.5 Pro (MMLU ~85.9%). Strengths: Often lead in general benchmarks and multimodal capabilities, backed by extensive R&D and proprietary data. Limitations: Opacity, API costs, potential vendor lock-in, less direct customizability.</li>
                        <li><strong>Open-Source Flagship Models:</strong> Examples: Meta Llama 3.1 70B/405B (MMLU ~86.0% for 70B Instruct), Alibaba Qwen2 72B (MMLU ~79.5% Instruct), DeepSeek-LLM 67B (MMLU ~75.7% Base), Mistral Large (API-first, MMLU ~81.2%). Strengths: Rapidly improving performance, transparency (weights usually available), high customizability, strong community support. Limitations: Can trail absolute SOTA on some frontier tasks, resource-intensive to self-host/fine-tune largest variants.</li>
                        <li><strong>Open-Source Efficient & Specialized Models:</strong> Examples: Google Gemma 2 9B/27B, Microsoft Phi-3 series, Mistral 7B/8x7B (Mixtral), Qwen1.5 0.5B-14B. Strengths: Excellent performance-per-parameter, suitable for on-device/local deployment, lower inference costs, strong for specific tasks or as fine-tuning bases. Limitations: Lower raw capabilities compared to flagship models. (Many available via Hugging Face, Ollama).</li>
                    </ul>

                    <h2>The Expanding Realm of Agentic AI: Customization vs. Integrated Platforms</h2>
                    <p>AI agents capable of planning, tool use, and executing multi-step tasks are increasingly built upon LLMs. Open models like Llama 3, Qwen, or Mistral offer high flexibility for developers to create bespoke agentic frameworks, enabling deep integration with custom tools and data. Closed platforms (e.g., OpenAI's Assistants API with function calling, Anthropic's tool use capabilities, Google's Vertex AI Agent Builder) provide more integrated and often polished agentic environments but may have restrictions on the level of control and customization. The choice influences innovation velocity, accessibility, and the types of agentic systems that can be developed.</p>
                </article>
                <aside class="sidebar">
                    <div class="sidebar-item">
                        <h4>LLM Release Timeline (Cumulative Major Families/Series)</h4>
                        <div class="graph-container"><canvas id="timelineOpenClosedChart"></canvas></div>
                        <p>Tracks cumulative releases of significant open and closed model families/versions (e.g., GPT series, Llama series, Claude series, Gemini series, Qwen series, Mistral series).</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Parameter Count Comparison (Selected Models, Billions)</h4>
                        <div class="graph-container"><canvas id="parameterCountChart"></canvas></div>
                        <p>Bar chart showing approximate or stated parameter counts for representative open and closed models. Note: "T" denotes Trillions.</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Spectrum of Openness (Illustrative Model Distribution)</h4>
                        <div class="graph-container"><canvas id="opennessSpectrumChart"></canvas></div>
                        <p>Categories: Full Open (Permissive License, Weights, Code, Data) → Weights & Code (Community/Research License) → Weights Only (Restricted) → API Access Only → Fully Closed (Internal).</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Key Model Families & Developers</h4>
                        <p><strong>Closed:</strong> OpenAI (GPT series), Anthropic (Claude series), Google (Gemini series).</p>
                        <p><strong>Open/Mixed:</strong> Meta (Llama series), Mistral AI (Mistral, Mixtral, Large), Alibaba (Qwen series), Google (Gemma series), DeepSeek AI (DeepSeek Coder/LLM), Stability AI (Stable Beluga), EleutherAI (Pythia), TII (Falcon).</p>
                    </div>
                </aside>
            </div>
        </div>
    </div>

    <!-- TAB 2: Capabilities & Reasoning -->
    <div id="capabilities" class="tab-content">
        <div class="mac-window-title-bar"><span class="mac-window-title">Capabilities & Reasoning</span></div>
        <div class="content-wrapper">
            <h1>LLM Capabilities & Reasoning: Open vs. Closed Deep Dive</h1>
            <div class="container">
                <article class="main-content">
                    <h2>Advanced Reasoning, Factuality, and Controllability</h2>
                    <p>Techniques like Chain-of-Thought (CoT), Self-Consistency, and newer methods like Tree-of-Thoughts or Graph-of-Thoughts are employed to enhance complex reasoning in LLMs. Open models (e.g., Llama 3, Qwen2, specialized math models like DeepSeekMath) allow researchers to dissect and improve reasoning pathways more directly due to model access. Closed models (e.g., GPT-4o, Claude 3 Opus, Gemini 1.5 Pro) often exhibit state-of-the-art reasoning, but their internal mechanisms are opaque. Factuality remains a challenge; while techniques like Retrieval Augmented Generation (RAG) and improved alignment help, hallucination persists. Controllability via system prompts and structured outputs is improving across the board, but open models offer deeper avenues for modification.</p>

                    <h2>Multimodal Prowess and Contextual Understanding</h2>
                    <p>Frontier closed models such as Google's Gemini series, OpenAI's GPT-4o, and Anthropic's Claude 3 family demonstrate strong multimodal capabilities, processing and generating text, images, audio, and sometimes video. Open-source multimodal models (e.g., LLaVA, Qwen-VL, IDEFICS2) are rapidly advancing, offering strong alternatives, particularly for research and custom applications. Long-context understanding is another key battleground. Models like Gemini 1.5 Pro (up to 2M tokens via API), Claude 3 (200K), and GPT-4o (128K) lead in closed-source. Open models like Llama 3.1 405B (128K+) and Qwen2 (128K+) are catching up, enabling new applications requiring extensive contextual information.</p>

                    <h2>Performance Across Standardized Benchmarks</h2>
                    <p>Performance on benchmarks like MMLU (general knowledge), HumanEval/MBPP (coding), GSM8K (math reasoning), and various safety/alignment benchmarks (e.g., BBQ, ToxiGen) are key indicators.
                    <ul>
                        <li><strong>Closed Models:</strong> Consistently achieve top scores on broad benchmarks. GPT-4o reported 88.7% on MMLU, 90.2% on HumanEval. Claude 3 Opus: MMLU 86.8%, HumanEval 84.9%. Gemini 1.5 Pro: MMLU 85.9%, HumanEval 83.7%.</li>
                        <li><strong>Open Models:</strong> Show remarkable progress. Llama 3.1 70B Instruct reached ~86.0% on MMLU, 81.7% on HumanEval. Qwen2 72B Instruct ~79.5% MMLU, 78.0% HumanEval. DeepSeek-LLM 67B base ~75.7% MMLU, 78.7% HumanEval (for code variant).</li>
                    </ul>
                    The gap is narrowing, especially for well-resourced open models, and community fine-tuning often pushes open models to SOTA in specific domains. However, closed models still often hold an edge on the most comprehensive, cutting-edge multimodal and reasoning tasks due to scale and proprietary data/techniques.
                    </p>
                </article>
                <aside class="sidebar">
                    <div class="sidebar-item">
                        <h4>Reasoning & Factuality Transparency Potential (Illustrative)</h4>
                        <div class="graph-container"><canvas id="reasoningTransparencyChart"></canvas></div>
                        <p>Open Models: Higher (weights/code access allows deeper inspection). Closed Models: Lower (API/output only limits direct study of internal reasoning).</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Key Benchmark Score Comparison (Selected Models)</h4>
                        <div class="graph-container"><canvas id="benchmarkPerformanceChart"></canvas></div>
                        <p>Compares leading open and closed models on MMLU, HumanEval (Pass@1), and GSM8K (Pass@1). Data from official sources, ~Mid 2024-Q1 2025.</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Context Window Leaders (Max Tokens, K)</h4>
                        <div class="graph-container"><canvas id="contextWindowChart"></canvas></div>
                        <p>Illustrates maximum context window sizes for selected models (e.g., Gemini 1.5 Pro, Claude 3 Opus, GPT-4o, Llama 3.1 405B, Qwen2 72B Long).</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Multimodal Capabilities (Selected)</h4>
                        <p><strong>Closed SOTA:</strong> GPT-4o (Text, Image, Audio I/O), Gemini 1.5 Pro (Text, Image, Audio, Video I/O), Claude 3 (Text, Image Input).</p>
                        <p><strong>Open Examples:</strong> Qwen-VL Plus/Max, LLaVA-NeXT, IDEFICS2 8B (Text, Image I/O). Some support for audio/video emerging.</p>
                    </div>
                </aside>
            </div>
        </div>
    </div>
    
    <!-- TAB 3: Training & Data -->
    <div id="training" class="tab-content">
        <div class="mac-window-title-bar"><span class="mac-window-title">Training: Data, Methods, Costs</span></div>
        <div class="content-wrapper">
            <h1>Training & Evolution: Data Sources, Methodologies, and Economics</h1>
            <div class="container">
                <article class="main-content">
                    <h2>The Data Foundation: Scale, Curation, and Proprietary Assets</h2>
                    <p>Pre-training LLMs consumes vast quantities of data, often in the tens of trillions of tokens. Common sources include public web scrapes (e.g., Common Crawl, C4), curated open datasets (e.g., The Pile, RedPajama, FineWeb), books, code repositories, and scientific articles. Leading closed-model developers also leverage extensive proprietary datasets, which can contribute to performance advantages but reduce transparency. Data quality, diversity, deduplication, and rigorous cleaning are paramount for model performance and safety. For example, Llama 3 was trained on over 15 trillion tokens, and Qwen2 on over 3 trillion. The cost of acquiring, processing, and filtering petabytes of data is substantial for all SOTA efforts.</p>
                    
                    <h2>Training Techniques: From SFT to Advanced Alignment and MoE</h2>
                    <p>The typical LLM training lifecycle involves:
                    <ol>
                        <li><strong>Pre-training:</strong> Self-supervised learning on massive unlabeled text/multimodal corpora to learn general representations.</li>
                        <li><strong>Supervised Fine-Tuning (SFT):</strong> Training on smaller, high-quality datasets of instruction-response pairs to teach specific behaviors and styles.</li>
                        <li><strong>Preference Alignment:</strong> Techniques like Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), or newer methods like Constitutional AI (used by Anthropic) and Reinforcement Learning from AI Feedback (RLAIF) are used to align models with human preferences, helpfulness, and harmlessness. Meta's Llama 3, for instance, utilized a combination of SFT, rejection sampling, PPO (for RLHF), and DPO.</li>
                    </ol>
                    Architectural innovations like Mixture-of-Experts (MoE), seen in models like Mixtral 8x7B and rumored for GPT-4, allow scaling parameter counts while keeping inference costs manageable by only activating a subset of "experts" per token. Training MoE models presents unique challenges in load balancing and expert specialization.</p>

                    <h2>Synthetic Data and the Challenge of Model Collapse</h2>
                    <p>The increasing prevalence of AI-generated text on the internet poses a risk of "model collapse" or "Habsburg AI," where models trained on their own outputs can degrade in quality or diversity over time. Both open and closed communities are actively researching mitigation strategies, including careful data filtering, a focus on high-quality human-generated data, and techniques to identify and manage synthetic data in training sets. Some models, like Llama 3, explicitly state efforts to filter out data from other major model providers. The use of synthetic data for SFT and preference alignment, however, is common and can be beneficial if carefully curated.</p>
                </article>
                <aside class="sidebar">
                    <div class="sidebar-item">
                        <h4>Illustrative Pre-Training Data Composition (Volume %)</h4>
                        <div class="graph-container"><canvas id="trainingDataSourcesChart"></canvas></div>
                        <p>Distribution: Public Web Scrapes (e.g., Common Crawl), Curated Open Datasets (Books, Code, Academic), Proprietary Corporate Data, Public Instruction/Alignment Data.</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Estimated Training Costs (Millions USD, Order of Magnitude)</h4>
                        <div class="graph-container"><canvas id="trainingCostChart"></canvas></div>
                        <p>Compares illustrative costs for SOTA Closed (e.g., GPT-4/Claude3 Opus class), Large Open (Llama 3 70B/Qwen2 72B class), and Mid-Size Open (Mistral 7B/Phi-3 Medium class).</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Pre-Training Data Size (Total Tokens - Selected Models)</h4>
                        <div class="graph-container"><canvas id="trainingDataSizeChart"></canvas></div>
                        <p>Llama 3 (15T+), GPT-4 (est. ~13T), Qwen2 (3T+), Claude 3 (est. ~5T), Gemma Series (2-6T), DeepSeek Series (2-4.5T). Data from model cards/papers.</p>
                    </div>
                </aside>
            </div>
        </div>
    </div>

    <!-- TAB 4: Ecosystem & Values -->
    <div id="ecosystem" class="tab-content">
        <div class="mac-window-title-bar"><span class="mac-window-title">Ecosystem & Values</span></div>
        <div class="content-wrapper">
            <h1>Human-AI Ecosystem: Trust, Values, and Collaborative Dynamics</h1>
            <div class="container">
                <article class="main-content">
                    <h2>Transparency, Trustworthiness, and Accountability</h2>
                    <p>Model openness significantly impacts trust and accountability. Open-source models, by allowing inspection of (to varying degrees) code, weights, and sometimes training data methodologies, facilitate greater transparency. This enables community-driven auditing, vulnerability discovery, and development of diverse safety tools (e.g., Llama Guard, NeMo Guardrails). Closed "black-box" models require greater reliance on the vendor's internal processes, safety claims, and external audits (where conducted). Principles of "Humble AI"—acknowledging model limitations and uncertainties—are arguably easier to implement and verify with open systems where mechanisms can be directly studied and modified.</p>
                    
                    <h2>Value Alignment: Diverse Philosophies and Implementations</h2>
                    <p>The "Value Generalization Problem"—ensuring AI infers and acts upon the breadth of human values, not just demonstrated preferences from limited data—is a core challenge for both paradigms. Open, community-driven alignment efforts (e.g., using diverse public datasets for DPO/RLHF, crowd-sourcing preference data) can reflect a wider range of values but may lack centralized coherence or rigorous safety testing at scale. Corporate-led alignment in closed models (e.g., Anthropic's Constitutional AI, OpenAI's extensive RLHF with dedicated red-teaming) benefits from focused resources and structured safety protocols but may embed the values of a smaller group of developers or the corporation itself. The definition and implementation of "harmlessness" can vary significantly.</p>

                    <h2>Innovation Ecosystems: Accessibility, Competition, and Economic Models</h2>
                    <p>Open-source LLMs, readily available via platforms like Hugging Face, Ollama, and various model repositories, dramatically lower barriers to entry for researchers, startups, and developers globally. This fosters broad innovation, customization for niche applications, and competition. Closed models, while often leading in raw SOTA performance, gate access through APIs and pricing, which can shape innovation towards platform-specific solutions. The economic models differ: open-source thrives on community contributions, consultancy, specialized fine-tuning services, and infrastructure/hosting for open models. Closed-source relies on subscriptions, API usage fees, and enterprise contracts. The rise of strong open models is pressuring API pricing and fostering a more competitive market.</p>
                </article>
                <aside class="sidebar">
                    <div class="sidebar-item">
                        <h4>Comparative Trust & Utility Factors (Illustrative Scores 1-10)</h4>
                        <div class="graph-container"><canvas id="trustFactorsChart"></canvas></div>
                        <p>Radar chart comparing Open vs. Closed on: Transparency, Raw SOTA Performance, Cost-Effectiveness (Self-host/API), Community Audit Potential, Customizability, Data Privacy Control (Self-Host).</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Open Source License Distribution (Illustrative - Common Types)</h4>
                        <div class="graph-container"><canvas id="licenseDistributionChart"></canvas></div>
                        <p>Pie chart showing common OS licenses: Apache 2.0, Llama Community License, MIT License, Other Permissive (e.g., BigScience OpenRAIL-M), Restrictive/Non-Commercial.</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Key Open Source Hubs & Communities</h4>
                        <p><strong>Hugging Face:</strong> Central platform for models, datasets, tools (Transformers, Diffusers, etc.).</p>
                        <p><strong>Ollama:</strong> Simplifies local deployment of open LLMs.</p>
                        <p><strong>GitHub:</strong> Hosts countless open-source AI projects and model implementations.</p>
                        <p><strong>LAION:</strong> Known for large open image-text datasets (e.g., LAION-5B).</p>
                         <p><strong>EleutherAI:</strong> Research collective producing open models (Pythia) and datasets (The Pile).</p>
                    </div>
                </aside>
            </div>
        </div>
    </div>
    
    <!-- TAB 5: Applications & Impact -->
    <div id="applications" class="tab-content">
        <div class="mac-window-title-bar"><span class="mac-window-title">Applications & Impact</span></div>
        <div class="content-wrapper">
            <h1>Applications & Impact: Open vs. Closed LLMs in the Real World</h1>
            <div class="container">
                <article class="main-content">
                    <h2>Democratizing Advanced AI: From Research Labs to Local Deployments</h2>
                    <p>Open-source models like Llama 3, Mistral 7B, Qwen1.5, and Phi-3 are empowering a wide array of users—from individual developers building local applications with Ollama to researchers and startups creating specialized solutions without hefty API fees. This contrasts with API-centric access for closed models like GPT-4o or Claude 3, which offer ease of use and often SOTA general capabilities but less control and higher costs at scale. This democratization accelerates AI adoption in education, non-profits, small businesses, and resource-constrained environments, fostering global innovation.</p>
                    
                    <h2>Deep Customization, Domain Specialization, and Enhanced Privacy</h2>
                    <p>A primary advantage of open models is deep customization. Businesses can fine-tune models like Llama 3 or Qwen2 on proprietary data for specific industry needs (e.g., finance, legal, healthcare, scientific research) while maintaining data privacy by hosting locally or in a secure private cloud. This allows for tailored solutions that better understand domain-specific jargon, entities, and workflows. Closed models offer some fine-tuning capabilities (often "managed fine-tuning" where data is sent to the provider), but the level of control and data residency options are typically more limited compared to self-hosting an open model.</p>

                    <h2>Economic Models, Market Competition, and Societal Considerations</h2>
                    <p>The proliferation of capable open-source LLMs is fostering intense competition, driving innovation, and potentially lowering costs for AI-powered services. It challenges the dominance of a few large AI labs and creates opportunities for new businesses offering specialized open-source solutions or MLOps platforms for open models. However, the ease of access to powerful open models also raises concerns about misuse (e.g., generation of misinformation, malicious code, or non-consensual content) if safety guardrails are insufficient or easily bypassed. This necessitates ongoing research into robust safety mechanisms, responsible release strategies, and clear ethical guidelines for both open and closed ecosystems.</p>
                </article>
                <aside class="sidebar">
                    <div class="sidebar-item">
                        <h4>Cost of Inference: API vs. Self-Hosted Open (Illustrative)</h4>
                        <div class="graph-container"><canvas id="inferenceCostChart"></canvas></div>
                        <p>Compares hypothetical relative monthly costs for processing a high volume of tokens via a leading Closed API vs. self-hosting an efficient Open LLM (factoring in infrastructure and operational overhead).</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>LLM Use Case Suitability Matrix (Illustrative Scores 1-10)</h4>
                        <div class="graph-container"><canvas id="useCaseSuitabilityChart"></canvas></div>
                        <p>Comparing Open vs. Closed for: Academic Research, Enterprise Customization, General Consumer Applications, Sensitive Data Applications, Frontier SOTA Tasks.</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Popular Local LLM Deployment Tools</h4>
                        <p><strong>Ollama:</strong> Simplifies running LLMs like Llama 3, Mistral, Qwen locally.</p>
                        <p><strong>LM Studio:</strong> GUI for discovering, downloading, and running local LLMs.</p>
                        <p><strong>Jan.ai:</strong> Open-source alternative to LM Studio for local LLM operation.</p>
                        <p><strong>Hugging Face Transformers:</strong> Core library for model loading and inference, often used under the hood.</p>
                    </div>
                </aside>
            </div>
        </div>
    </div>

    <!-- TAB 6: Current Outlook (Mid-2025) -->
    <div id="outlook" class="tab-content">
        <div class="mac-window-title-bar"><span class="mac-window-title">Current Outlook (Mid-2025)</span></div>
        <div class="content-wrapper">
            <h1>Current Outlook: Navigating a Landscape of Rapid Advancements & Complex Challenges</h1>
            <div class="container">
                <article class="main-content">
                    <h2>Tackling Core LLM Deficiencies: Hallucination, Bias, and Robust Safety</h2>
                    <p>The entire LLM field, both open and closed, is intensely focused on mitigating fundamental challenges. Factual hallucination remains a persistent issue, with RAG, improved fact-checking pre/post-generation, and better alignment being key research areas. Embedded biases from vast training data continue to require sophisticated detection and mitigation techniques. Ensuring robust safety against misuse, adversarial attacks, and emergent harmful behaviors is a top priority. Openness allows for broader community red-teaming and diverse safety tool development, while closed systems rely on internal rigor and, increasingly, external safety evaluations pre-release. The debate over which approach yields "safer" AI is ongoing and complex.</p>
                    
                    <h2>The Unfolding Quest for Generalization, Interpretability, and Efficiency</h2>
                    <p>Achieving robust generalization beyond training distributions and mastering true common-sense reasoning are grand challenges. While scaling laws (performance improving with model size, data, and compute) continue to hold, researchers are exploring new architectures, training paradigms, and data enrichment strategies. Interpretability and explainability of LLM decision-making remain elusive for large models, hindering trust and debugging. Concurrently, significant effort is dedicated to improving model efficiency through quantization, pruning, knowledge distillation, and optimized inference engines to make powerful LLMs more accessible and sustainable.</p>

                    <h2>Market Dynamics: Fierce Competition, Coexistence, and Emerging Standards</h2>
                    <p>The LLM market is characterized by fierce competition. Open-source models are rapidly closing performance gaps on many standard benchmarks, sometimes even surpassing older closed SOTA models. A pattern of coexistence is clear: closed models often provide cutting-edge general-purpose APIs and multimodal capabilities, while open models excel in customization, research, cost-sensitive applications, and fostering specialized ecosystems. Hybrid approaches (e.g., fine-tuning open models with proprietary data, then using them alongside closed APIs for specific tasks) are common. Standardization efforts for model evaluation, safety testing, and interoperability are emerging but still nascent.</p>
                </article>
                <aside class="sidebar">
                    <div class="sidebar-item">
                        <h4>AI Safety & Alignment Focus (Illustrative Effort/Success Index)</h4>
                        <div class="graph-container"><canvas id="aiSafetyFocusChart"></canvas></div>
                        <p>Illustrative scores reflecting relative R&D investment and perceived success in safety/alignment for Open Ecosystems (Community, Tools) vs. Closed Labs (Internal Teams, Policy).</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Overall Capability Trend (Leading Models - Illustrative Index)</h4>
                        <div class="graph-container"><canvas id="currentCapabilityChart"></canvas></div>
                        <p>Tracks general capability advancement (aggregate of benchmarks, features) for top-tier open and closed models from early 2024 to mid-2025.</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Key LLM Development Challenges (Mid-2025)</h4>
                        <ul>
                            <li><strong>Data:</strong> Sustainable access to diverse, high-quality, and rights-cleared data. Managing synthetic data.</li>
                            <li><strong>Compute:</strong> Scaling training & inference efficiently amid rising demand and hardware limitations/costs.</li>
                            <li><strong>Alignment & Safety:</strong> Robustly aligning with nuanced human values, preventing misuse, ensuring controllability.</li>
                            <li><strong>Evaluation:</strong> Developing comprehensive, dynamic benchmarks beyond static leaderboards; evaluating real-world utility.</li>
                            <li><strong>Efficiency:</strong> Reducing computational footprint (training & inference) without sacrificing performance.</li>
                            <li><strong>Multimodality:</strong> Seamlessly integrating and reasoning across diverse data types.</li>
                            <li><strong>Reasoning:</strong> Improving complex, multi-step, and common-sense reasoning.</li>
                        </ul>
                    </div>
                </aside>
            </div>
        </div>
    </div>

    <!-- TAB 7: Future Projections (2027) -->
    <div id="projections" class="tab-content">
        <div class="mac-window-title-bar"><span class="mac-window-title">Future Projections (2027)</span></div>
        <div class="content-wrapper">
            <h1>Future Projections: The LLM Landscape Towards Year-End 2027</h1>
            <div class="report-header">
                <p><strong>Extrapolating from current trajectories, anticipated breakthroughs, and evolving market dynamics, this section projects the state of open vs. closed LLMs by year-end 2027. These projections consider advancements in model capabilities, hardware, data practices, ethical frameworks, and societal responses.</strong></p>
                <p>"Projections are informed by the S-curve of AI adoption, ongoing research trends in areas like scaling, efficiency, alignment, and multimodality, and potential shifts in regulatory landscapes."</p>
                <p class="report-date">Projection Target: Year-End 2027</p>
            </div>
            <div class="container">
                <article class="main-content">
                    <h2>Capabilities & Performance: Broad Parity in General Tasks, Specialized Edges</h2>
                    <p>By 2027, flagship general-purpose open-source LLMs are projected to achieve near-parity with, or even exceed, contemporary closed-source counterparts on a wide range of established benchmarks (e.g., MMLU, HumanEval, GSM8K scores likely >95-97% for top-tier models of both types). The ease of fine-tuning and architectural adaptation will solidify open models as SOTA in numerous niche domains (e.g., specific scientific fields, low-resource languages, specialized coding). Closed-source labs, leveraging massive compute and potentially unique proprietary datasets or architectural breakthroughs (e.g., in neuromorphic approaches, advanced MoE, or new forms of memory/reasoning), may still maintain an edge in pioneering entirely new capabilities, extreme-scale multimodality, or tasks requiring unprecedented long-context reasoning (e.g., >10-20M token effective context, full video understanding). Highly optimized open models (e.g., 10B-70B parameter class) will be ubiquitous for edge, local, and real-time applications, with significantly improved efficiency.</p>
                    
                    <h2>Accessibility, Cost Structures, and Democratization of AGI-like Tools</h2>
                    <p>Open-source models will be pervasively accessible via mature tooling for local deployment, federated learning, and on-device execution, even on high-end consumer hardware. The "cost-to-capability" ratio for self-hosted and community-supported open models will be exceptionally attractive, further democratizing access to powerful AI. Closed-source API pricing will likely become more granular and competitive, with premium tiers for frontier capabilities and enterprise-grade SLAs. The "digital divide" may shift from access to basic LLM capability to access to: (1) massive compute for training/fine-tuning very large open models from scratch, (2) highly specialized or continuously updated proprietary datasets, and (3) the most advanced, potentially AGI-proximal, closed APIs.</p>

                    <h2>Market Dynamics: Hybrid Strategies, Open-Core Dominance, and Vertical AI</h2>
                    <p>Hybrid AI strategies will become standard in enterprise: open-source for baseline capabilities, customization, data privacy, and cost control, complemented by closed-source APIs for tasks demanding absolute SOTA, unique multimodal features, or access to massive, curated knowledge bases. The "open-core" model will likely dominate, where powerful base models are released openly (perhaps with a delay after initial proprietary access), and commercial entities (including original developers) offer value-added services like enterprise support, managed fine-tuning, specialized data pipelines, and deployment solutions. We anticipate a Cambrian explosion of "Vertical AI" solutions: highly specialized LLMs (often open-source based) deeply integrated into specific industry workflows (e.g., AI for drug discovery, AI legal assistants, AI for chip design).</p>

                    <h2>Ethical AI, Governance, Data Ecosystem, and Societal Integration</h2>
                    <p>International AI regulations and standards (e.g., building on EU AI Act, NIST AI RMF) will be more established by 2027, significantly impacting development, deployment, and auditing for both open and closed models. Robust frameworks for AI safety, security, and provenance will be critical. Open models will benefit from community-driven safety tools and diverse red-teaming, but ensuring consistent safety across a decentralized ecosystem will remain a challenge. Closed models will operate under stringent corporate governance and regulatory compliance, offering more centralized (though not fully transparent) safety and accountability mechanisms. Data governance, including synthetic data labeling, copyright solutions for training data, and privacy-enhancing technologies, will be central. LLMs will be deeply integrated into many aspects of work, education, and daily life, raising ongoing societal discussions about job displacement, cognitive reliance, and the human-AI relationship.</p>
                </article>
                <aside class="sidebar">
                    <div class="sidebar-item">
                        <h4>Projected SOTA Capability (MMLU Equivalent - 2027)</h4>
                        <div class="graph-container"><canvas id="projectedCapabilityChart2027"></canvas></div>
                        <p>Line chart showing projected MMLU-equivalent aggregate scores for top-tier Open vs. Closed LLMs converging towards 2027, with both likely exceeding 97%+, closed potentially retaining a slight edge or leading in novel capabilities not captured by MMLU.</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Projected Enterprise LLM Deployment Mix (% Usage/Spend - 2027)</h4>
                        <div class="graph-container"><canvas id="deploymentModelsChart2027"></canvas></div>
                        <p>Pie chart: Local/On-Premise Open, Managed Cloud Open (PaaS/SaaS), Public API (Closed), Hybrid (Integrated Open + Closed API solutions).</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Projected Dominant LLM Innovation Drivers (Relative Impact - 2027)</h4>
                        <div class="graph-container"><canvas id="innovationDriversChart2027"></canvas></div>
                        <p>Bar chart: Open Source Ecosystem (incl. community, startups building on open models), Big Tech R&D (Closed & Open contributions), Academic Research, Hardware & Systems Co-design (e.g., AI accelerators, neuromorphic).</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Projected AI Training Compute & Efficiency (2027)</h4>
                        <div class="graph-container"><canvas id="aiComputeDemandChart2027"></canvas></div>
                        <p>Dual-axis line chart: Training SOTA Frontier Models (ExaFLOP-days/model) vs. Avg. Training for Competitive Open Models (PetaFLOP-days/model). Expect SOTA compute to rise, but efficiency gains for mainstream open models too.</p>
                    </div>
                    <div class="sidebar-item">
                        <h4>Projected Specialization vs. Generalization Focus (R&D Effort % - 2027)</h4>
                        <div class="graph-container"><canvas id="specializationGeneralizationChart2027"></canvas></div>
                        <p>Illustrative balance: Open-source heavily driving specialized/vertical AI; Closed-source pushing frontiers of general-purpose AGI-like systems, but also offering specialized enterprise solutions.</p>
                    </div>
                </aside>
            </div>
        </div>
    </div>

    <footer class="footer">
        <p>AI Horizon (70s Autumn Retro Edition - Fixed) is a conceptual project presenting a data-informed perspective on the open vs. closed LLM landscape. Information and statistics are drawn from or inspired by academic research and public knowledge, including data from sources like model provider websites, arXiv, ArtificialAnalysis.ai, and industry reports.</p>
        <p><strong>Core Inspirations:</strong> Academic papers on LLM architecture, training, and evaluation; official releases from AI labs; reputable AI news and analysis. This is a thematic synthesis for illustrative purposes.</p>
        <p>Retro Design & Thematic Reinterpretation by AI.</p>
    </footer>

    
<script>
        // GLOBAL SCOPE FUNCTIONS for tab and modal control
        // These MUST be defined before the DOMContentLoaded listener,
        // or before the browser parses HTML elements with onclick attributes if those attributes are present.
        function openTab(evt, tabName) {
            let i, tabcontent, tabbuttons;
            tabcontent = document.getElementsByClassName("tab-content");
            for (i = 0; i < tabcontent.length; i++) {
                tabcontent[i].classList.remove("active");
            }
            tabbuttons = document.getElementsByClassName("tab-button");
            for (i = 0; i < tabbuttons.length; i++) {
                tabbuttons[i].classList.remove("active");
            }
            const currentTabContent = document.getElementById(tabName);
            if (currentTabContent) {
                currentTabContent.classList.add("active");
            }
            if (evt && evt.currentTarget) {
                evt.currentTarget.classList.add("active");
            }
            window.scrollTo(0, 0);
        }

        function openModal(modalId) {
            const modal = document.getElementById(modalId);
            if (modal) {
                modal.style.display = "block";
            }
        }

        function closeModal(modalId) {
            const modal = document.getElementById(modalId);
            if (modal) {
                modal.style.display = "none";
            }
        }
        
        // This must also be in the global scope if modals are to be closed by clicking outside
        window.onclick = function(event) {
            const modals = document.getElementsByClassName('modal');
            for (let i = 0; i < modals.length; i++) {
                if (event.target === modals[i]) { 
                    modals[i].style.display = "none";
                }
            }
        };

        // DOMContentLoaded for chart initializations and other DOM-dependent setup
        document.addEventListener('DOMContentLoaded', () => {
            // Ensure the first tab is active on load
            const firstTabButton = document.querySelector('.tab-button.active'); // Already marked active in HTML
            if (firstTabButton) {
                const onclickAttr = firstTabButton.getAttribute('onclick');
                const tabNameMatch = onclickAttr ? onclickAttr.match(/openTab\s*\(\s*event\s*,\s*'([^']+)'\s*\)/) : null;
                if (tabNameMatch && tabNameMatch[1]) {
                    const firstTabName = tabNameMatch[1];
                    const firstTabContent = document.getElementById(firstTabName);
                    if (firstTabContent && !firstTabContent.classList.contains('active')) { 
                        firstTabContent.classList.add('active'); // Ensure content matches button
                    }
                }
            }
            
            const rootStyle = getComputedStyle(document.documentElement);
            const chartFont = rootStyle.getPropertyValue('--font-chart-label').trim() || 'Arial';
            const chartTextColor = rootStyle.getPropertyValue('--text-primary').trim();
            const colorGridTheme = rootStyle.getPropertyValue('--chart-grid-color-theme').trim();

            const colorOpenPri = rootStyle.getPropertyValue('--chart-color-open-pri').trim();
            const colorOpenSec = rootStyle.getPropertyValue('--chart-color-open-sec').trim();
            const colorClosedPri = rootStyle.getPropertyValue('--chart-color-closed-pri').trim();
            const colorClosedSec = rootStyle.getPropertyValue('--chart-color-closed-sec').trim();
            const colorProjected = rootStyle.getPropertyValue('--chart-color-projected').trim();
            const colorMisc1 = rootStyle.getPropertyValue('--chart-color-misc-1').trim();
            const colorMisc2 = rootStyle.getPropertyValue('--chart-color-misc-2').trim();
            const colorWindowBg = rootStyle.getPropertyValue('--bg-color-window').trim();
            
            // Retrieve CSS variables for tooltip correctly
            const tooltipBgColor = rootStyle.getPropertyValue('--button-bg').trim();
            const tooltipBorderColor = rootStyle.getPropertyValue('--border-color').trim();


            const commonChartOptions = {
                responsive: true, maintainAspectRatio: false, animation: { duration: 0 },
                scales: {
                    x: { ticks: { color: chartTextColor, font: { family: chartFont, size: 8 } }, grid: { color: colorGridTheme }, title: { display: false, font: {family: chartFont, size: 9}}},
                    y: { ticks: { color: chartTextColor, font: { family: chartFont, size: 8 }, beginAtZero: true }, grid: { color: colorGridTheme }, title: { display: false, font: {family: chartFont, size: 9}}},
                    y1: { display: false } 
                },
                plugins: {
                    legend: { display: false, labels: {color: chartTextColor, font: {size:8, family: chartFont}, usePointStyle: true, boxWidth: 5, padding: 8} },
                    tooltip: { 
                        backgroundColor: tooltipBgColor, // Correctly use retrieved value
                        titleColor: chartTextColor, 
                        bodyColor: chartTextColor, 
                        borderColor: tooltipBorderColor, // Correctly use retrieved value
                        borderWidth: 1, 
                        titleFont: { family: chartFont, weight: 'bold', size: 10 }, 
                        bodyFont: { family: chartFont, size: 9 }, 
                        padding: 6
                    },
                    title: { display: false, color: chartTextColor, font: {family: chartFont, size: 10, weight: 'bold'}, padding: { top: 4, bottom:7}}
                }
            };

            function createChart(canvasId, type, data, customOptions = {}) {
                const canvasElement = document.getElementById(canvasId);
                if(!canvasElement) { console.error("AI HORIZON DEBUG: Canvas element not found for ID:", canvasId); return; }
                const chartCtx = canvasElement.getContext('2d');
                if(!chartCtx) { console.error("AI HORIZON DEBUG: Failed to get 2D context for canvas ID:", canvasId); return;}

                const existingChart = Chart.getChart(canvasElement); 
                if (existingChart) {
                    existingChart.destroy();
                }

                const options = JSON.parse(JSON.stringify(commonChartOptions)); 
                if (customOptions.scales) {
                    if (customOptions.scales.x) Object.assign(options.scales.x, customOptions.scales.x);
                    if (customOptions.scales.y) Object.assign(options.scales.y, customOptions.scales.y);
                    if (customOptions.scales.y1) {
                        options.scales.y1 = JSON.parse(JSON.stringify(customOptions.scales.y1)); 
                        options.scales.y1.display = true; 
                        if(!options.scales.y1.ticks) options.scales.y1.ticks = {};
                         if(!options.scales.y1.ticks.font) options.scales.y1.ticks.font = {};
                        options.scales.y1.ticks.font.family = chartFont; 
                        options.scales.y1.ticks.font.size = 8;
                        options.scales.y1.ticks.color = chartTextColor;
                         if(!options.scales.y1.title) options.scales.y1.title = {};
                        options.scales.y1.title.font = {family: chartFont, size:9};
                    }
                }
                if (customOptions.plugins?.legend) Object.assign(options.plugins.legend, customOptions.plugins.legend);
                if (customOptions.plugins?.title) Object.assign(options.plugins.title, customOptions.plugins.title);
                if (customOptions.indexAxis) options.indexAxis = customOptions.indexAxis;
                
                try {
                    new Chart(chartCtx, { type, data, options });
                } catch (e) {
                    console.error("AI HORIZON DEBUG: Error creating chart for ID:", canvasId, e);
                }
            }

            // --- CHART DEFINITIONS (using new color variables) ---
            // TAB 1: Foundations
            createChart('timelineOpenClosedChart', 'line', { 
                labels: [new Date(2022,0).toISOString(), new Date(2022,6).toISOString(), new Date(2023,0).toISOString(), new Date(2023,6).toISOString(), new Date(2024,0).toISOString(), new Date(2024,6).toISOString(), new Date(2025,0).toISOString()],
                datasets: [ 
                    { label: 'Closed Model Families (Cum.)', data: [2, 2, 3, 5, 7, 9, 10], borderColor: colorClosedPri, backgroundColor: colorClosedPri + '33', fill: false, tension: 0.1, pointRadius: 2.5 }, 
                    { label: 'Open Model Families (Cum.)', data: [1, 2, 4, 7, 11, 15, 18], borderColor: colorOpenPri, backgroundColor: colorOpenPri + '33', fill: false, tension: 0.1, pointRadius: 2.5 } 
                ]
            }, { plugins: { legend: { display: true, position: 'top'}, title: {display: true, text: 'LLM Release Timeline (Cumulative Major Families)'}}, scales: { x: {type: 'time', time: {unit: 'quarter', tooltipFormat: 'MMM yyyy'}, title: {display: true, text: 'Release Date'}}, y: { title: { display: true, text: '# Cum. Major Model Series'}}}});
            
            createChart('parameterCountChart', 'bar', { 
                labels: ["GPT-4o (C)", "Claude 3 Op (C)", "Llama 3.1 405B (O)", "Qwen2 72B (O)", "DeepSeek 67B (O)", "Mixtral 8x7B (O)", "Gemma 2 27B (O)"],
                datasets: [{ label: 'Parameters (Billions)', data: [1800, 250, 405, 72, 67, 46.7, 27], backgroundColor: [colorClosedPri, colorClosedSec, colorOpenPri, colorOpenSec, colorMisc1, colorMisc2, colorOpenPri] }]
            }, { plugins: { title: {display: true, text: 'Parameter Counts (Billions - Stated/Est.)'}, legend: {display:false}}, scales: {y: {type: 'logarithmic', title: {display: true, text: 'Parameters (Billions)'}}}});
            
            createChart('opennessSpectrumChart', 'bar', { 
                 labels: ["Full Open (Permissive, Assets)", "Weights & Code (Comm. Lic.)", "Weights Only (Research/NC)", "API Only (Closed Weights)", "Fully Closed (Internal)"],
                datasets: [{ label: 'Illustrative Model Count', data: [12, 18, 8, 20, 7], backgroundColor: [colorOpenPri, colorOpenSec, colorMisc1, colorClosedPri, colorClosedSec] }]
            }, { indexAxis: 'y', plugins: { title: { display: true, text: 'LLM Access Types (Illustrative Model Count)'}, legend: {display: false}}, scales: {x: {title: {display: true, text: 'Est. # Prominent Model Types'}}}});

            // TAB 2: Capabilities
            createChart('reasoningTransparencyChart', 'doughnut', { 
                labels: ["Open Models (High Potential)", "Closed Models (Low Public Potential)"],
                datasets: [{ data: [78, 22], backgroundColor: [colorOpenPri, colorClosedPri], borderColor: colorWindowBg, borderWidth: 2}]
            }, { plugins: { legend: { display: true, position: 'top'}, title: {display: true, text: 'Reasoning Transparency Potential'}}});
            
            createChart('benchmarkPerformanceChart', 'bar', { 
                labels: ["GPT-4o", "Claude 3 Opus", "Gemini 1.5 Pro", "Llama 3.1 70B(I)", "Qwen2 72B(I)", "DeepSeek-LLM 67B(I)", "Mistral Large", "Grok-1.5"],
                datasets: [ 
                    { label: 'MMLU (%)', data: [88.7, 86.8, 85.9, 86.0, 79.5, 75.7, 81.2, 73.0], backgroundColor: colorClosedSec }, 
                    { label: 'HumanEval (Pass@1 %)', data: [90.2, 84.9, 83.7, 81.7, 78.0, 78.7, null, 63.2], backgroundColor: colorOpenSec }, 
                    { label: 'GSM8K (Pass@1 %)', data: [95.0, 90.7, 92.9, 88.4, 84.0, 85.0, null, 65.0], backgroundColor: colorMisc1 }
                ]
            }, { indexAxis: 'y', plugins: { title: {display: true, text: 'Key Benchmark Scores'}, legend: {display: true, position: 'bottom'}}, scales: {x: {title: {display:true, text:'Score (%) - Data: Various Sources, ~Mid 2024-Q1 2025'}, suggestedMin: 50}}});
            
            createChart('contextWindowChart', 'bar', { 
                 labels: ["Gemini 1.5 Pro (C)", "Claude 3 Opus (C)", "GPT-4o (C)", "Llama 3.1 405B (O)", "Qwen2 72B (O, Long)", "Mistral Large (API/C)"],
                datasets: [{ label: 'Max Context Window (K Tokens)', data: [1024, 200, 128, 128, 128, 32], backgroundColor: [colorClosedPri, colorClosedSec, colorClosedPri, colorOpenPri, colorOpenSec, colorClosedSec] }]
            }, { indexAxis: 'y', plugins: { title: {display: true, text: 'Max Context Window Leaders (K Tokens)'}, legend: {display: false}}, scales: {x: {type: 'logarithmic', title: {display: true, text: 'Tokens (Thousands)'}}}});

            // TAB 3: Training
            createChart('trainingDataSourcesChart', 'pie', { 
                 labels: ["Public Web Scrapes (CC, C4)", "Curated Open Data (Books, Code, ArXiv)", "Proprietary Corporate Data", "Instruction/Alignment Data (Public/Synthetic)"],
                datasets: [{ data: [42, 28, 20, 10], backgroundColor: [colorOpenPri, colorOpenSec, colorClosedSec, colorMisc2], borderColor: colorWindowBg, borderWidth: 2}]
            }, { plugins: {legend: {display: true, position: 'top'}, title: {display: true, text: 'Pre-Training Data Mix (Est. Volume %)'}}});
            
            createChart('trainingCostChart', 'bar', { 
                labels: ["Frontier SOTA (GPT-4/Claude3 Opus Class)", "Large Open (Llama3 70B/Qwen2 72B Class)", "Mid-Size Open (Mistral 7B/Phi-3 Med Class)"],
                datasets: [{ label: 'Est. Training Cost (Millions USD)', data: [120, 18, 1.8], backgroundColor: [colorClosedPri, colorOpenPri, colorOpenSec] }]
            }, { plugins: {title: {display:true, text:'Est. Training Costs (Millions USD)'}}, scales: {y: {type: 'logarithmic', title: {display: true, text: 'Cost in Millions USD (Order of Magnitude)'}}}});
            
            createChart('trainingDataSizeChart', 'bar', { 
                labels: ["Llama 3", "GPT-4 (est.)", "Qwen2", "Claude 3 (est.)", "Gemma Series", "DeepSeek Series"],
                datasets: [{ label: 'Pretraining Data Size (Trillions of Tokens)', data: [15, 13, 3.5, 5, 6, 4.5], backgroundColor: [colorOpenPri, colorClosedPri, colorOpenSec, colorClosedSec, colorMisc2, colorMisc1] }]
            }, { plugins: {title: {display: true, text: 'Pretraining Data Size (Trillions of Tokens)'}, legend: {display: false}}, scales: {y: {title: {display: true, text: 'Tokens (Trillions)'}}}});

            // TAB 4: Ecosystem
            createChart('trustFactorsChart', 'radar', { 
                 labels: ["Transparency", "SOTA Performance", "Cost-Effectiveness", "Community Audit", "Customizability", "Data Privacy (Self-Host)"],
                datasets: [ 
                    { label: 'Open LLMs', data: [8.5, 8, 9, 8.5, 9, 9], borderColor: colorOpenPri, backgroundColor: colorOpenPri+'66', pointBackgroundColor: colorOpenPri, pointBorderColor: colorWindowBg, pointHoverBackgroundColor: colorWindowBg, pointHoverBorderColor: colorOpenPri}, 
                    { label: 'Closed LLMs', data: [3, 9.5, 6.5, 3, 5.5, 7], borderColor: colorClosedPri, backgroundColor: colorClosedPri+'66', pointBackgroundColor: colorClosedPri, pointBorderColor: colorWindowBg, pointHoverBackgroundColor: colorWindowBg, pointHoverBorderColor: colorClosedPri} 
                ]
            }, { plugins: {legend: {display: true, position: 'top'}, title: {display: true, text: 'Trust & Utility Factors (Scores 1-10)'}}, scales: {r: {angleLines: { color: colorGridTheme }, grid: { color: colorGridTheme }, pointLabels: {font: {size:8}, color: chartTextColor}, suggestedMin: 0, suggestedMax:10, ticks: {stepSize: 2, backdropColor: 'transparent', color: chartTextColor, font: {size: 7}}}}});
            
            createChart('licenseDistributionChart', 'pie', { 
                labels: ["Apache 2.0", "Llama Comm. Lic.", "MIT License", "Other Permissive OS", "Restrictive OS (e.g. NC)"],
                datasets: [{ data: [45, 20, 10, 15, 10], backgroundColor: [colorOpenPri, colorOpenSec, colorMisc1, colorMisc2, colorClosedSec], borderColor: colorWindowBg, borderWidth: 2 }]
            }, { plugins: {legend: {display:true, position:'top'}, title: {display:true, text: 'Common Open Source License Types'}}});

            // TAB 5: Applications
            createChart('inferenceCostChart', 'bar', { 
                labels: ["Closed API (High Volume)", "Self-Host Efficient Open", "Local Deploy Open (Low Volume)"],
                datasets: [{label: 'Relative Cost Index per 1M Tokens', data: [100, 15, 3], backgroundColor: [colorClosedPri, colorOpenPri, colorOpenSec] }]
            }, { plugins: {title:{display:true, text:'Relative Inference Cost Index (Illustrative)'}, legend:{display:false}}, scales: {y: {title:{display:true, text:'Cost Index'}}}});
            
            createChart('useCaseSuitabilityChart', 'bar', { 
                labels: ["Research & Edu", "Enterprise Custom", "Gen. Consumer App", "Sensitive Data App", "Frontier SOTA Tasks"],
                datasets: [ 
                    { label: 'Open LLMs', data: [9.5, 9, 7.5, 9.5, 7.5], backgroundColor: colorOpenPri }, 
                    { label: 'Closed LLMs', data: [6.5, 7.5, 9.5, 6.5, 9.5], backgroundColor: colorClosedPri } 
                ]
            }, { plugins: {title:{display:true, text:'Use Case Suitability (Score 1-10, Illustrative)'}, legend:{display:true, position:'top'}}, scales: {y: {title:{display:true, text:'Suitability Score'}, suggestedMax:10}}});

            // TAB 6: Outlook
            createChart('aiSafetyFocusChart', 'bar', { 
                labels: ["Open Ecosystem (Research, Tools, Audit)", "Closed Labs (Internal Teams, Policy)"],
                datasets: [{label: 'Focus/Resource on AI Safety & Alignment', data: [78, 92], backgroundColor: [colorOpenPri, colorClosedPri] }]
            }, { plugins: {title:{display:true, text:'AI Safety & Alignment Focus (Illustrative Effort/Success)'}, legend:{display:false}}, scales: {y: {title:{display:true, text:'Focus/Success Index (0-100)'}, suggestedMax:100}}});
            
            createChart('currentCapabilityChart', 'line', { 
                labels: ["Q1 '24", "Q2 '24", "Q3 '24", "Q4 '24", "Q1 '25", "Q2 '25"],
                 datasets: [ 
                    { label: 'Leading Closed Models (Agg. Capability)', data: [90, 91.5, 93, 94, 95, 95.5], borderColor: colorClosedPri, backgroundColor: colorClosedPri + '33', fill:false, tension: 0.1, pointRadius: 2.5}, 
                    { label: 'Leading Open Models (Agg. Capability)', data: [84, 86.5, 89, 91, 93, 94], borderColor: colorOpenPri, backgroundColor: colorOpenPri + '33', fill:false, tension: 0.1, pointRadius: 2.5}
                ]
            }, { plugins: { legend: { display: true, position: 'top'}, title: {display: true, text: 'Overall Capability Trend (Illustrative Index)'}}, scales: { y: { title: { display: true, text: 'Capability Index (0-100)'}, suggestedMax:100, suggestedMin:80}}});

            // TAB 7: Projections
            createChart('projectedCapabilityChart2027', 'line', { 
                labels: ["Mid-2025", "End-2025", "Mid-2026", "End-2026", "Mid-2027", "End-2027"],
                datasets: [ 
                    { label: 'Top Tier Closed LLM (Proj. Agg. Score)', data: [95.5, 96.2, 96.8, 97.3, 97.8, 98.2], borderColor: colorClosedPri, backgroundColor: colorClosedPri + '33', borderDash: [5, 5], fill: false, tension: 0.1, pointRadius: 2.5 }, 
                    { label: 'Top Tier Open LLM (Proj. Agg. Score)', data: [94, 94.8, 95.5, 96.2, 96.8, 97.2], borderColor: colorOpenPri, backgroundColor: colorOpenPri + '33', fill: false, tension: 0.1, pointRadius: 2.5 } 
                ]
            }, { plugins: { legend: { display: true, position: 'top'}, title: {display: true, text: 'Projected Agg. Capability Scores (2027)'}}, scales: {y: {title: {display: true, text: 'Capability Index (Illustrative)'}, suggestedMin: 90, suggestedMax: 100}} });
            
            createChart('deploymentModelsChart2027', 'doughnut', { 
                labels: ["Local/On-Prem Open", "Managed Cloud Open", "Public API (Closed)", "Hybrid (Open + Closed API)"],
                datasets: [{ data: [45, 20, 15, 20], backgroundColor: [colorOpenPri, colorOpenSec, colorClosedPri, colorProjected], borderColor: colorWindowBg, borderWidth: 2}]
            }, { plugins: { legend: { display: true, position: 'top'}, title: {display: true, text: 'Projected Enterprise LLM Deployment (2027)'}} });
            
            createChart('innovationDriversChart2027', 'bar', { 
                labels: ["Open Source Ecosystem", "Big Tech R&D (Overall)", "Academia & Startups", "Hardware & Systems Co-design"],
                datasets: [{ label: "Relative Impact", data: [8.8, 9.2, 7.8, 8.5], backgroundColor: [colorOpenPri, colorClosedSec, colorProjected, colorMisc2] }]
            }, { plugins: { title: {display: true, text: 'Key Innovation Drivers (2027 Impact)'}, legend: {display: false}}, scales: {y: {title: {display:true, text:'Impact Score (1-10)'}, suggestedMax:10}}});
            
            createChart('aiComputeDemandChart2027', 'line', { 
                 labels: ["2024", "2025", "2026", "2027"],
                datasets: [ 
                    { label: 'Training SOTA Frontier Models (ExaFLOP-days)', data: [200, 400, 800, 1600], borderColor: colorClosedPri, backgroundColor: colorClosedPri + '33', fill:false, tension: 0.1, yAxisID: 'y', pointRadius: 2.5}, 
                    { label: 'Avg. Training Competitive Open Models (PetaFLOP-days)', data: [100, 180, 300, 500], borderColor: colorOpenPri, backgroundColor: colorOpenPri + '33', fill:false, tension: 0.1, yAxisID: 'y1', pointRadius: 2.5} 
                ]
            }, { plugins: { legend: { display: true, position: 'top'}, title: {display: true, text: 'Projected AI Training Compute Demand (2027)'}},
                 scales: { 
                    y: { type: 'logarithmic', position: 'left', title: {display: true, text: 'SOTA Training (ExaFLOP-days est.)'}, ticks: {color: colorClosedPri, font:{size:7}}}, 
                    y1: { type: 'logarithmic', position: 'right', title: {display: true, text: 'Open Model Training (PetaFLOP-days est.)'}, grid: {drawOnChartArea: false}, ticks: {color: colorOpenPri, font:{size:7}}} 
                } 
            });
            
            createChart('specializationGeneralizationChart2027', 'bar', { 
                labels: ['Open Source LLMs', 'Closed Source LLMs'],
                datasets: [ 
                    { label: 'Focus on Specialized Models (e.g. Code, Math, Science)', data: [90, 55], backgroundColor: colorOpenSec}, 
                    { label: 'Focus on General-Purpose Frontier Models', data: [60, 95], backgroundColor: colorClosedSec} 
                ]
            }, { plugins: {title:{display:true, text: 'Projected Model Focus by 2027 (% R&D Effort)'}, legend:{display:true, position:'top'}}, scales:{y:{title:{display:true, text:'% R&D Effort (Illustrative)'}, suggestedMax:100}}});


            console.log("AI Horizon (70s Autumn Retro Edition - Fixed) charts initialized.");
        });
    </script>
</body>
</html>
